Reason to using both mse and mae and where

Why use both MSE and MAE
Role		Function		Reason
Loss (MSE)	Guides model training	Smooth, differentiable, penalizes large errors — helps training converge.

Metric (MAE)	Evaluates performance	More interpretable and robust — good for comparing models in your results table.

Mean Squared Error (MSE) was used as the training loss because it provides a smooth and differentiable objective that heavily penalizes large deviations, encouraging the network to learn stable regression mappings. Mean Absolute Error (MAE) was included as a complementary evaluation metric for interpretability, indicating the average magnitude of prediction errors.


The Optimizer (ADAM)
The optimizer is the algorithm that updates the network’s weights (the parameters the model learns) based on how much error it made.

Smarter version of gradient descent
There are several optimizers in deep learning — here’s a quick overview:

Optimizer		Key idea	Pros	Cons
SGD (Stochastic Gradient Descent)	Updates weights in the direction of the average gradient.	Simple, mathematically clear.	Slow, needs careful learning rate tuning.

Momentum	Adds a “velocity” term that smooths updates.	Faster convergence, avoids oscillations.	Still needs manual tuning.
RMSProp	Adjusts learning rate individually for each weight.	Adapts well to noisy data.	Can overshoot minima.

Adam (Adaptive Moment Estimation)	Combines Momentum + RMSProp.	Fast, adaptive, usually works “out of the box”.	Slightly more memory use, can sometimes generalize less well if tuned poorly.